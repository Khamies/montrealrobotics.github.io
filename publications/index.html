<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width initial-scale=1" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
      
        <title>Robotics Group @ University of Montreal | Publications</title>
        <meta name="description" content="The Robotics and Embodied AI Lab @ U de Montreal">
      
        <link rel="shortcut icon" href="http://montrealrobotics.github.io/assets/img/favicon.png">
        <link rel="stylesheet"
              href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta.2/css/bootstrap.min.css"
              integrity="sha384-PsH8R72JQ3SOdhVi3uxftmaW6Vc51MKb0q5P2rRUpPvrszuE4W1povHYgTpBfshb"
              crossorigin="anonymous">
        <link rel="stylesheet" href="http://montrealrobotics.github.io/assets/css/sb-admin-2.min.css">
        <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.8.1/css/all.css" integrity="sha384-50oBUHEmvpQ+1lW4y57PTFmhCaXp0ML5d60M1M7uH2+nqUivzIebhndOJK28anvf" crossorigin="anonymous">
        <link rel="stylesheet" href="http://montrealrobotics.github.io/assets/css/main.css">
        <link rel="canonical" href="http://montrealrobotics.github.io/publications/">

      </head>
    <body>
        <div class="container">
            <!-- This is a bit nasty, but it basically says be a column first, and on larger screens be a spaced out row -->
            <nav class="navbar sticky-top navbar-expand-lg navbar-light navbar-dark bg-primary mb-2 text-white">
                <a class="navbar-brand" href="/">REAL</a>
                <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                        <span class="navbar-toggler-icon"></span>
                      </button>
                      <div class="collapse navbar-collapse" id="navbarNav">
                        <ul class="navbar-nav">
                                
                                
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/">
                                            Home
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/people.html">
                                            People
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/research.html">
                                            Research
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link active"
                                           href="/publications/">
                                            Publications
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/contact.html">
                                            Contact
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="">
                                            Blog
                                        </a>
                                    </li>
                                    
            
                                    
            
                                    
                                    
                                    
            
                                    <li class="nav-item">
                                        <a class="nav-link "
                                           href="/blog.html">
                                            
                                        </a>
                                    </li>
                                    
                        </ul>
                      </div>
            </nav>

            

            <!-- 
                <h1>Publications</h1>
             -->

            <div class="card border-bottom-primary shadow py-2 mb-4">
        <div class="card-body">
          <div class="row no-gutters align-items-center">
            <div class="col mr-2">
              <div class="h2 font-weight-bold text-primary mb-1">Publications</div>
            </div>
          </div>
        </div>
</div>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2019</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="bharadhwaj2018data">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/homanga2019icra.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies</b></span>
    <span class="author">
      
      	
        
          
            
              Homanga Bharadhwaj,
            
          
        
      
      	
        
          
            
              Zihan Wang,
            
          
        
      
      	
        
          
            
              Yoshua Bengio,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em></em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1810.04871" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Learning effective visuomotor policies for robots purely from data is challenging, but also appealing since a learning-based system should not require manual tuning or calibration. In the case of a robot operating in a real environment the training process can be costly, time-consuming, and even dangerous since failures are common at the start of training. For this reason, it is desirable to be able to leverage \textitsimulation and \textitoff-policy data to the extent possible to train the robot. In this work, we introduce a robust framework that plans in simulation and transfers well to the real environment. Our model incorporates a gradient-descent based planning module, which, given the initial image and goal image, encodes the images to a lower dimensional latent state and plans a trajectory to reach the goal. The model, consisting of the encoder and planner modules, is trained through a meta-learning strategy in simulation first. We subsequently perform adversarial domain transfer on the encoder by using a bank of unlabelled but random images from the simulation and real environments to enable the encoder to map images from the real and simulated environments to a similarly distributed latent representation. By fine tuning the entire model (encoder + planner) with far fewer real world expert demonstrations, we show successful planning performances in different navigation tasks.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ bharadhwaj2018data, <br />
      author = {  
                  
                      Bharadhwaj, Homanga and
                    
                   
                  
                      Wang, Zihan and
                    
                   
                  
                      Bengio, Yoshua and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { A Data-Efficient Framework for Training and Sim-to-Real Transfer of Navigation Policies }, <br />
      
      
        journal = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="sai2019dal">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/dal.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Deep Active Localization</b></span>
    <span class="author">
      
      	
        
          
            
              Sai Krishna,
            
          
        
      
      	
        
          
            
              Keehong Seo,
            
          
        
      
      	
        
          
            
              Dhaivat Bhatt,
            
          
        
      
      	
        
          
            
              Vincent Mai,
            
          
        
      
      	
        
          
            
              Krishna Murthy,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em></em>
    
    
      2019
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1903.01669" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
    <a href="https://github.com/montrealrobotics/dal" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Active localization is the problem of generating robot actions that allow it to maximally disambiguate its pose within a reference map. Traditional approaches to this use an information-theoretic criterion for action selection and hand-crafted perceptual models. In this work we propose an end-to-end differentiable method for learning to take informative actions that is trainable entirely in simulation and then transferable to real robot hardware with zero refinement. The system is composed of two modules: a convolutional neural network for perception, and a deep reinforcement learned planning module. We introduce a multi-scale approach to the learned perceptual model since the accuracy needed to perform action selection with reinforcement learning is much less than the accuracy needed for robot control. We demonstrate that the resulting system outperforms using the traditional approach for either perception or planning. We also demonstrate our approaches robustness to different map configurations and other nuisance parameters through the use of domain randomization in training. The code is also compatible with the OpenAI gym framework, as well as the Gazebo simulator.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ sai2019dal, <br />
      author = {  
                  
                      Krishna, Sai and
                    
                   
                  
                      Seo, Keehong and
                    
                   
                  
                      Bhatt, Dhaivat and
                    
                   
                  
                      Mai, Vincent and
                    
                   
                  
                      Murthy, Krishna and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Deep Active Localization }, <br />
      
      
        journal = { IEEE Robotics and Automation Letters (RAL) }, <br />
      
      
        year = { 2019 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2018</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2018probabilistic">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/paull2018coverage.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Probabilistic cooperative mobile robot area coverage and its application to autonomous seabed mapping</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Mae Seto,
            
          
        
      
      	
        
          
            
              John J Leonard,
            
          
        
      
      	
        
          and
          
            
              Howard Li
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>The International Journal of Robotics Research</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>There are many applications that require mobile robots to autonomously cover an entire area with a sensor or end effector. The vast majority of the literature on this subject is focused on addressing path planning for area coverage under the assumption that the robot’s pose is known or that error is bounded. In this work, we remove this assumption and develop a completely probabilistic representation of coverage. We show that coverage is guaranteed as long as the robot pose estimates are consistent, a much milder assumption than zero or bounded error. After formally connecting robot sensor uncertainty with area coverage, we propose an adaptive sliding window filter pose estimator that provides a close approximation to the full maximum a posteriori estimate with a computation cost that is bounded over time. Subsequently, an adaptive planning strategy is presented that automatically exploits conditions of low vehicle uncertainty to more efficiently cover an area. We further extend this approach to the multi-robot case where robots can communicate through a (possibly faulty and low-bandwidth) channel and make relative measurements of one another. In this case, area coverage is achieved more quickly since the uncertainty over the robots’ trajectories is reduced. We apply the framework to the scenario of mapping an area of seabed with an autonomous underwater vehicle. Experimental results support the claim that our method achieves guaranteed complete coverage notwithstanding poor navigational sensors and that resulting path lengths required to cover the entire area are shortest using the proposed cooperative and adaptive approach.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ paull2018probabilistic, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Seto, Mae and
                    
                   
                  
                      Leonard, John J and
                    
                   
                  
                      Li, Howard 
                    
                   }, <br />
      title = { Probabilistic cooperative mobile robot area coverage and its application to autonomous seabed mapping }, <br />
      
        journal = { The International Journal of Robotics Research }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="ort2018icra">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/ort2018icra.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps</b></span>
    <span class="author">
      
      	
        
          
            
              Teddy Ort,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://toyota.csail.mit.edu/sites/default/files/documents/papers/ICRA2018_AutonomousVehicleNavigationRuralEnvironment.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>State-of-the-art autonomous driving systems rely heavily on detailed and highly accurate prior maps. However, outside of small urban areas, it is very challenging to build, store, and transmit detailed maps since the spatial scales are so large. Furthermore, maintaining detailed maps of large rural areas can be impracticable due to the rapid rate at which these environments can change. This is a significant limitation for the widespread applicability of autonomous driving technology, which has the potential for an incredibly positive societal impact. In this paper, we address the problem of autonomous navigation in rural environments through a novel mapless driving framework that combines sparse topological maps for global navigation with a sensor-based perception system for local navigation. First, a local navigation goal within the sensor view of the vehicle is chosen as a waypoint leading towards the global goal. Next, the local perception system generates a feasible trajectory in the vehicle frame to reach the waypoint while abiding by the rules of the road for the segment being traversed. These trajectories are updated to remain in the local frame using the vehicle’s odometry and the associated uncertainty based on the least-squares residual and a recursive filtering approach, which allows the vehicle to navigate road networks reliably, and at high speed, without detailed prior maps. We demonstrate the performance of the system on a full-scale autonomous vehicle navigating in a challenging rural environment and benchmark the system on a large amount of collected data.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ ort2018icra, <br />
      author = {  
                  
                      Ort, Teddy and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Autonomous Vehicle Navigation in Rural Environments without Detailed Prior Maps }, <br />
      
      
        journal = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="mai2018local">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/mai2018blimp.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Local Positioning System Using UWB Range Measurements for an Unmanned Blimp</b></span>
    <span class="author">
      
      	
        
          
            
              Vincent Mai,
            
          
        
      
      	
        
          
            
              Mina Kamel,
            
          
        
      
      	
        
          
            
              Matthias Krebs,
            
          
        
      
      	
        
          
            
              Andreas Schaffner,
            
          
        
      
      	
        
          
            
              Daniel Meier,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Roland Siegwart
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>IEEE Robotics and Automation Letters</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://ieeexplore.ieee.org/document/8392389" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Unmanned blimps are a safe and reliable alternative to conventional drones when flying above people. On-board real-time tracking of their pose and velocities is a necessary step toward autonomous navigation. There is a need for an easily deployable technology that is able to accurately and robustly estimate the pose and velocities of a blimp in 6 DOF, as well as unexpected applied forces and torques, in an uncontrolled environment. We present two multiplicative extended Kalman filters using ultrawideband radio sensors and a gyroscope to address this challenge. One filter is updated using a dynamics model of the blimp, whereas the other uses a constant speed model. We describe a set of experiments in which these estimators have been implemented on an embedded flight controller. They were tested and compared in accuracy and robustness in a hardware-in-loop simulation as well as on a real blimp. This approach can be generalized to any lighter than air robot to track it with the necessary accuracy, precision, and robustness to allow autonomous navigation.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ mai2018local, <br />
      author = {  
                  
                      Mai, Vincent and
                    
                   
                  
                      Kamel, Mina and
                    
                   
                  
                      Krebs, Matthias and
                    
                   
                  
                      Schaffner, Andreas and
                    
                   
                  
                      Meier, Daniel and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Siegwart, Roland 
                    
                   }, <br />
      title = { Local Positioning System Using UWB Range Measurements for an Unmanned Blimp }, <br />
      
        journal = { IEEE Robotics and Automation Letters }, <br />
      
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="CTCNet">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/ctcnet.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Geometric Consistency for Self-Supervised End-to-End Visual Odometry</b></span>
    <span class="author">
      
      	
        
          
            
              Ganesh Iyer,
            
          
        
      
      	
        
          
            
              J Krishna Murthy,
            
          
        
      
      	
        
          
            
              K Gunshi Gupta,
            
          
        
      
      	
        
          and
          
            
              Liam Paull
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In CVPR Workshop on Deep Learning for Visual SLAM</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1804.03789" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
    <a href="https://krrish94.github.io/CTCNet-release/" target="_blank" class="buttonPP">Project Page</a>
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>With the success of deep learning based approaches in tackling challenging problems in computer vision, a wide range of deep architectures have recently been proposed for the task of visual odometry (VO) estimation. Most of these proposed solutions rely on supervision, which requires the acquisition of precise ground-truth camera pose information, collected using expensive motion capture systems or high-precision IMU/GPS sensor rigs. In this work, we propose an unsupervised paradigm for deep visual odometry learning. We show that using a noisy teacher, which could be a standard VO pipeline, and by designing a loss term that enforces geometric consistency of the trajectory, we can train accurate deep models for VO that do not require ground-truth labels. We leverage geometry as a self-supervisory signal and propose "Composite Transformation Constraints (CTCs)", that automatically generate supervisory signals for training and enforce geometric consistency in the VO estimate. We also present a method of characterizing the uncertainty in VO estimates thus obtained. To evaluate our VO pipeline, we present exhaustive ablation studies that demonstrate the efficacy of end-to-end, self-supervised methodologies to train deep models for monocular VO. We show that leveraging concepts from geometry and incorporating them into the training of a recurrent neural network results in performance competitive to supervised deep VO methods.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ CTCNet, <br />
      author = {  
                  
                      Iyer, Ganesh and
                    
                   
                  
                      Murthy, J Krishna and
                    
                   
                  
                      Gunshi Gupta, K and
                    
                   
                  
                      Paull, Liam 
                    
                   }, <br />
      title = { Geometric Consistency for Self-Supervised End-to-End Visual Odometry }, <br />
      
      
        journal = { CVPR Workshop on Deep Learning for Visual SLAM }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="amini2018learning">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/amini2018parallel.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Learning steering bounds for parallel autonomous systems</b></span>
    <span class="author">
      
      	
        
          
            
              Alexander Amini,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Thomas Balch,
            
          
        
      
      	
        
          
            
              Sertac Karaman,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2018
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/https://dspace.mit.edu/handle/1721.1/117632" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Deep learning has been successfully applied to “end-to-end” learning of the autonomous driving task, where a deep neural network learns to predict steering control commands from camera data input. While these previous works support reactionary control, the representation learned is not usable for higher-level decision making required for autonomous navigation. This paper tackles the problem of learning a representation to predict a continuous control probability distribution, and thus steering control options and bounds for those options, which can be used for autonomous navigation. Each mode of the distribution encodes a possible macro-action that the system could execute at that instant, and the covariances of the modes place bounds on safe steering control values. Our approach has the added advantage of being trained on unlabeled data collected from inexpensive cameras. The deep neural network based algorithm generates a probability distribution over the space of steering angles, from which we leverage Variational Bayesian methods to extract a mixture model and compute the different possible actions in the environment. A bound, which the autonomous vehicle must respect in our parallel autonomy setting, is then computed for each of these actions. We evaluate our approach on a challenging dataset containing a wide variety of driving conditions, and show that our algorithm is capable of parameterizing Gaussian Mixture Models for possible actions, and extract steering bounds with a mean error of only 2 degrees. Additionally, we demonstrate our system working on a full scale autonomous vehicle and evaluate its ability to successful handle various different parallel autonomy situations.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ amini2018learning, <br />
      author = {  
                  
                      Amini, Alexander and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Balch, Thomas and
                    
                   
                  
                      Karaman, Sertac and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Learning steering bounds for parallel autonomous systems }, <br />
      
      
        journal = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2018 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2017</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2017duckietown">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/paull2017duckietown.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Duckietown: an open, inexpensive and flexible platform for autonomy education and research</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Jacopo Tani,
            
          
        
      
      	
        
          
            
              Heejin Ahn,
            
          
        
      
      	
        
          
            
              Javier Alonso-Mora,
            
          
        
      
      	
        
          
            
              Luca Carlone,
            
          
        
      
      	
        
          
            
              Michal Cap,
            
          
        
      
      	
        
          
            
              Yu Fan Chen,
            
          
        
      
      	
        
          
            
              Changhyun Choi,
            
          
        
      
      	
        
          
            
              Jeff Dusek,
            
          
        
      
      	
        
          
            
              Yajun Fang,
            
          
        
      
      	
        
          and
          
            
               others
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://www.mit.edu/ hangzhao/papers/duckietown.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Duckietown is an open, inexpensive and flexible platform for autonomy education and research. The platform comprises small autonomous vehicles (“Duckiebots”) built from off-the-shelf components, and cities (“Duckietowns”) complete with roads, signage, traffic lights, obstacles, and citizens (duckies) in need of transportation. The Duckietown platform offers a wide range of functionalities at a low cost. Duckiebots sense the world with only one monocular camera and perform all processing onboard with a Raspberry Pi 2, yet are able to: follow lanes while avoiding obstacles, pedestrians (duckies) and other Duckiebots, localize within a global map, navigate a city, and coordinate with other Duckiebots to avoid collisions. Duckietown is a useful tool since educators and researchers can save money and time by not having to develop all of the necessary supporting infrastructure and capabilities. All materials are available as open source, and the hope is that others in the community will adopt the platform for education and research.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ paull2017duckietown, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Tani, Jacopo and
                    
                   
                  
                      Ahn, Heejin and
                    
                   
                  
                      Alonso-Mora, Javier and
                    
                   
                  
                      Carlone, Luca and
                    
                   
                  
                      Cap, Michal and
                    
                   
                  
                      Chen, Yu Fan and
                    
                   
                  
                      Choi, Changhyun and
                    
                   
                  
                      Dusek, Jeff and
                    
                   
                  
                      Fang, Yajun and
                    
                   
                  
                      others,  
                    
                   }, <br />
      title = { Duckietown: an open, inexpensive and flexible platform for autonomy education and research }, <br />
      
      
        journal = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="rosman2017hybrid">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/rosman2017hybrid.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Hybrid control and learning with coresets for autonomous vehicles</b></span>
    <span class="author">
      
      	
        
          
            
              Guy Rosman,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          and
          
            
              Daniela Rus
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>
    
    
      2017
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://people.csail.mit.edu/rosman/papers/ctrl_embedding.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Modern autonomous systems such as driverless vehicles need to safely operate in a wide range of conditions. A potential solution is to employ a hybrid systems approach, where safety is guaranteed in each individual mode within the system. This offsets complexity and responsibility from the individual controllers onto the complexity of determining discrete mode transitions. In this work we propose an efficient framework based on recursive neural networks and coreset data summarization to learn the transitions between an arbitrary number of controller modes that can have arbitrary complexity. Our approach allows us to efficiently gather annotation data from the large-scale datasets that are required to train such hybrid nonlinear systems to be safe under all operating conditions, favoring underexplored parts of the data. We demonstrate the construction of the embedding, and efficient detection of switching points for autonomous and nonautonomous car data. We further show how our approach enables efficient sampling of training data, to further improve either our embedding or the controllers</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ rosman2017hybrid, <br />
      author = {  
                  
                      Rosman, Guy and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Rus, Daniela 
                    
                   }, <br />
      title = { Hybrid control and learning with coresets for autonomous vehicles }, <br />
      
      
        journal = { IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) }, <br />
      
      
        year = { 2017 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>

<div class="card border-left-primary shadow mb-1">
          <div class="card-body">
            <div class="h2 font-weight-bold text-primary mb-1">2016</div>
          </div>
  </div>
<ol class="bibliography"><li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="paull2016unified">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/paull2016unified.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>A Unified Resource-Constrained Framework for Graph SLAM</b></span>
    <span class="author">
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              Guoquan Huang,
            
          
        
      
      	
        
          and
          
            
              John J Leonard
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE International Conference on Robotics and Automation (ICRA)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/http://liampaull.ca/publications/Paull_ICRA_2016.pdf" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
    <a href="http://montrealrobotics.github.io/assets/pdf/http://liampaull.ca/publications/Paull_ICRA_2016_poster.pptx" target="_blank" class="buttonMM">Poster</a>
  
  
    <a href="http://montrealrobotics.github.io/assets/pdf/http://liampaull.ca/publications/Paull_ICRA_2016_presentation.pptx" target="_blank" class="buttonMM">Slides</a>
  
  
  
    <a href="https://github.com/liampaull/Resource_Constrained" target="_blank" class="buttonPP">Code</a>
  
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Graphical methods have proven an extremely useful tool employed by the mobile robotics community to frame estimation problems. Incremental solvers are able to process incoming sensor data and produce maximum a posteriori (MAP) estimates in realtime by exploiting the natural sparsity within the graph for reasonable-sized problems. However, to enable truly longterm operation in prior unknown environments requires algorithms whose computation, memory, and bandwidth (in the case of distributed systems) requirements scale constantly with time and environment size. Some recent approaches have addressed this problem through a two-step process - first the variables selected for removal are marginalized which induces density, and then the result is sparsified to maintain computational efficiency. Previous literature generally addresses only one of these two components. In this work, we attempt to explicitly connect all of the aforementioned resource constraint requirements by considering the node removal and sparsification pipeline in its entirety. We formulate the node selection problem as a minimization problem over the penalty to be paid in the resulting sparsification. As a result, we produce node subset selection strategies that are optimal in terms of minimizing the impact, in terms of Kullback-Liebler divergence (KLD), of approximating the dense distribution by a sparse one. We then show that one instantiation of this problem yields a computationally tractable formulation. Finally, we evaluate the method on standard datasets and show that the KLD is minimized as compared to other commonly-used heuristic node selection techniques.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ paull2016unified, <br />
      author = {  
                  
                      Paull, Liam and
                    
                   
                  
                      Huang, Guoquan and
                    
                   
                  
                      Leonard, John J 
                    
                   }, <br />
      title = { A Unified Resource-Constrained Framework for Graph SLAM }, <br />
      
      
        journal = { IEEE International Conference on Robotics and Automation (ICRA) }, <br />
      
      
        year = { 2016 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li>
<li><div id="publications">
<table style="width:100%" cellspacing="0">
  <col width="25%" />
  <col width="75%" />
<tr>



<div id="mu2016iros">

  <td style="border-left: 1px solid #000000; border-bottom: 1px solid #000000;">
  
  <img src="http://montrealrobotics.github.io/assets/img/papers/mu2016iros.png" style="width:100%;" />
  
  </td>

  <td style="border-bottom: 1px solid #000000; border-right: 1px solid #000000;">
  
    <span class="title"><b>Slam with objects using a nonparametric pose graph</b></span>
    <span class="author">
      
      	
        
          
            
              Beipeng Mu,
            
          
        
      
      	
        
          
            
              Shih-Yuan Liu,
            
          
        
      
      	
        
          
            
              Liam Paull,
            
          
        
      
      	
        
          
            
              John Leonard,
            
          
        
      
      	
        
          and
          
            
              Jonathan P How
            
          
        
      
    </span>

    <span class="periodical">
    
      <em>In IEEE/RSJ International Conference onnIntelligent Robots and Systems (IROS)</em>
    
    
      2016
    
    </span>
  

  <span class="links">
  
    <a class="abstract">Abstract</a>
  
  
    <a href="http://arxiv.org/abs/1704.05959" target="_blank" class="buttonTT">arXiv</a>
  
  
  
  
  
  
  
  
  
    <a href="https://www.youtube.com/watch?v=gOwMiFlj8KU" target="_blank" class="buttonSS">Video</a>
  

  <!-- BibTeX -->
  <a class="bibtex">BibTeX</a>

  </span>

  <!-- Hidden abstract block -->
  
  <div id="abstract">
  <span class="abstract hidden">
    <p>Mapping and self-localization in unknown environments are fundamental capabilities in many robotic applications. These tasks typically involve the identification of objects as unique features or landmarks, which requires the objects both to be detected and then assigned a unique identifier that can be maintained when viewed from different perspectives and in different images. The data association and simultaneous localization and mapping (SLAM) problems are, individually, well-studied in the literature. But these two problems are inherently tightly coupled, and that has not been well-addressed. Without accurate SLAM, possible data associations are combinatorial and become intractable easily. Without accurate data association, the error of SLAM algorithms diverge easily. This paper proposes a novel nonparametric pose graph that models data association and SLAM in a single framework. An algorithm is further introduced to alternate between inferring data association and performing SLAM. Experimental results show that our approach has the new capability of associating object detections and localizing objects at the same time, leading to significantly better performance on both the data association and SLAM problems than achieved by considering only one and ignoring imperfections in the other.</p>
  </span>
  </div>
  

  <!-- Hidden BibTeX block -->
  <div id="mybibtex">
  <span class="bibtex hidden">
    <p>
      @article{ mu2016iros, <br />
      author = {  
                  
                      Mu, Beipeng and
                    
                   
                  
                      Liu, Shih-Yuan and
                    
                   
                  
                      Paull, Liam and
                    
                   
                  
                      Leonard, John and
                    
                   
                  
                      How, Jonathan P 
                    
                   }, <br />
      title = { Slam with objects using a nonparametric pose graph }, <br />
      
      
        journal = { IEEE/RSJ International Conference onnIntelligent Robots and Systems (IROS) }, <br />
      
      
        year = { 2016 }, <br />
      
      }
    </p>
  </span>
  </div>

  </td>
</div>

</tr>
</table>

</div>
</li></ol>


            <div class="my-5 pt-5 text-muted text-center text-md">
                <h4>&copy; Robotics and Embodied AI Lab Team, 2019</h4>
                <p class="affiliations">
                        <a href="https://en.diro.umontreal.ca/home/"><img src="http://montrealrobotics.github.io/assets/img/diro.png" width="150px" alt=
                            "Department of Computer Science and Operations Research"> </a>
                        |
                        <a href="https://www.umontreal.ca/">
                            <img src="http://montrealrobotics.github.io/assets/img/udem.png" width="150px" alt=
                            "Universit&eacute; de Montr&eacute;al">
                            </a>
                </p>
                <h4>Powered by jekyll</h4>
            </div>

        </div> <!-- /container -->

        <!-- Load jquery-->
        <script src="//code.jquery.com/jquery-1.12.4.min.js"></script>
        <!-- Support retina images. -->
        <script type="text/javascript"
                src="/assets/js/srcset-polyfill.js"></script>
        <!-- Support hidden abstract/bibtex blocks. -->
        <script src="/assets/js/common.js"></script>
    </body>
</html>
